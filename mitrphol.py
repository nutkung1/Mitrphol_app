# -*- coding: utf-8 -*-
"""Mitrphol.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h5Zy8GOEyVo1-Vf5-1emHFgzo7HuXXKg
"""

!nvidia-smi

!pip install -Uqqq pip --progress-bar off
!pip install -qqq langchain==0.0.228 --progress-bar off
!pip install -qqq chromadb==0.3.26 --progress-bar off
!pip install -qqq sentence-transformers==2.2.2 --progress-bar off
!pip install -qqq auto-gptq==0.2.2 --progress-bar off
!pip install -qqq einops==0.6.1 --progress-bar off
!pip install -qqq unstructured==0.8.0 --progress-bar off
!pip install -qqq transformers==4.30.2 --progress-bar off
!pip install -qqq torch==2.0.1 --progress-bar off

from pathlib import Path

import torch
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
from langchain.chains import ConversationalRetrievalChain
from langchain.chains.question_answering import load_qa_chain
from langchain.document_loaders import DirectoryLoader
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import HuggingFacePipeline
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from transformers import AutoTokenizer, GenerationConfig, TextStreamer, pipeline, AutoModelForCausalLM

"""**Data**"""

questions_dir = Path("skyscanner")
questions_dir.mkdir(exist_ok=True, parents=True)

def write_file(question, answer, file_path):
  text = f"""
  Q: {question}
  A: {answer}
  """.strip()
  with Path(questions_dir / file_path).open("w") as text_file:
    text_file.write(text)

write_file(
    question="What is Mitr Phol",
    answer="""Mitr Phol is Thailand's and Asia's biggest sugar and bio-energy producer.
    Mitr Phol Sugar Corp is a privately owned group of companies, mainly owned by the Vongkusolkit family.
    As of 2014, Mitr Phol is ranked as the world's fifth largest sugar producer,
     and the largest producer in Asia. It is Thailand's largest sugar producer and
     the second largest in China through its joint venture company East Asia Sugar.
     In addition to Thailand and China, Mitr Phol has operations and investments in Lao PDR, Cambodia, and
     most recently Australia. Its key business units include sugar, wood substitute materials, and renewable energy.
    """.strip(),
    file_path="question_1.txt",
)

write_file(
    question="Vision of Mitr Phol",
    answer="""To be the world-class sugar and bio-based leader
            by combining our fully integrated agribusiness model
            With innovative technology and management talent
            to creat value for a sustainable life for all.
    """.strip(),
    file_path="question_2.txt",
)

write_file(
    question="Mitr Phol Business",
    answer="""
    Mitr Phol have many business, namely, Sugarcane Development Business, Sugar Business,
    Energy Business, Wood Substitute Material, International Business, and Other Business.
    """.strip(),
    file_path="question_3.txt",
)

write_file(
    question="What is Mitr Phol's Sugarcane Development Business",
    answer="""The objective of Sugarcane Development Business is to do researching about sugarcane to support and developsugarcane gene. Make sustainability to farmer and sugarcane farm. Furthermore, the research will increase the productivity from 7 to 8 tons per rai to 20 tons per rai. Improving irrigation Setting up irrigation systems in and around sugarcane farms. Choosing the Right TechnologyTo be used in 376 villages in communities around Mitr Phol Group 6 sugar mills. Lastly, minimizing Costs.
    """.strip(),
    file_path="question_4.txt",
)

write_file(
    question="What is Mitr Phol's sugarcane business",
    answer="""
    Mitr Phol Group, the world 3rd largest sugar producer, is committed to business development. To add value to the sugarcane and sugar industries And the business has continued for more than half a century. Mitr Phol Group pays attention to every step of production. Give consumers confidence in world class quality standards.
    Make Mitr Phol well known and gain the confidence of customers. both domestically and abroad.
    """.strip(),
    file_path="question_5.txt",
)

write_file(
    question="How many Mitr Phol factories are there",
    answer="""
    Currently there are 7 factories consisting of Established in 1983,
Mitr Phu Khiao Sugar Factory Chaiyaphum Province,
production capacity 42,000 tons of sugarcane/day. Established in 1990,
Mitr Phol Sugar Factory, Dan Chang, Suphanburi Province,
production capacity 49,000 tons of sugar cane/day. Established in 1990,
Mitr Phu Wiang Sugar Factory Khon Kaen Province,
production capacity 36,000 tons of sugarcane/day. Established in 1997,
Singburi Sugar Factory Singburi Province,
production capacity 15,000 tons of sugarcane/day. Established in 1997,
Mitr Kalasin Sugar Factory Kalasin Province,
production capacity 39,000 tons of sugar cane/day. Established in 2012,
Mitr Phu Luang Sugar Factory, Loei Province,
production capacity 40,000 tons of sugar cane/day. Established in 2019,
Amnat Charoen Mitr Sugar Factory Amnat Charoen Province,
production capacity 15,000 tons of sugarcane/day
    """.strip(),
    file_path="question_6.txt",
)

write_file(
    question="What is Mitr Phol's energy business",
    answer="""
    Mitr Phol want to be a zero waste company in a short period of time, so we decided to create Bio power brightening the world with clean energy.
    Ethanol a clean alternative that is environmentally friendly and sell them out to the Electricity Authority. Also, bring back
    the steam power and power the machine.
    """.strip(),
    file_path="question_7.txt",
)

write_file(
    question="What is Mitr Phol's Wood Substitute Material Business",
    answer="""
    Panel Plus formerly MP Particle Board was established in 1990 under Mitr Phol Group management. We are a leading company that produces particle boards, MDF (medium density fiberboard) and melamine faced-chip boards, all of which are substitute materials for wood. With state of the art technology and highly experienced management, we produce 300,000 cubic meters of particle boards every year, 300,000 cubic meters of MDF every year, and 23.5 million square meters of melamine faced chip board every year.
    We are recognized across Asia as a leader in the industry, serving the needs of both domestic and international customers.
    """.strip(),
    file_path="question_8.txt",
)

write_file(
    question="What is Mitr Phol's international business",
    answer="""
     Mitr Phol want to expand our production base and adding economic value to the world. Present, Mitr Phol have invest
     their sugar mill to increase the product in 3 countries are China, Laos, and Australia.
    """.strip(),
    file_path="question_9.txt",
)

write_file(
    question="What is Mitr Phol product",
    answer="""
    The major product of Mitr Phol are sugar, molasses, fertilizer, and Wood substitute material.
    """.strip(),
    file_path="question_10.txt",
)

write_file(
    question="How many employee are in the Mitr Phol company",
    answer="""
    In Mitr Phol company there are approximately 7,400 employees divided to male 5,500 persons and female 1,900 persons.
    Nevertheless, Mitr Phol Group has contributed to the community income by hiring people from the neighboring area where our factory is located. Our recruitment helps supports them with work, occupation promotion, leveraging their life quality, and improving good relationships with the community. Basic compensation is paid in compliance with the laws. Through Strong Community Driven by the Disabled Project , the Company also employs person with disabilities from the communities to facilitate them for having workplace near their home, generating income for the disabilities, enabling them to be self-independent. Our contribution includes the support in establishing the Disabled Club to extend the career opportunity for other person with disability within the same sub-district. Such employment will reduce the inequality,
    encouraging the self-esteem among the disabled to live in the society with happiness.
    """.strip(),
file_path="question_11.txt",
)

write_file(
    question="Innovation and Reseacrh center of Mitr Phol",
    answer="""
    Mitr Phol Group realizes the importance and economic potential of sugarcane,
    which is the starting raw material for the sugar industry and other related industries.
    We established the Mitr Phol Innovation & Research Center Company Limited in 1997 to drive research
    and technological development that will help achieve sustainable sugarcane production and prep the group for business expansion. As the group is cane and sugar business evolved into an energy and bio plastics business, we elevated our R&D department to the Mitr Phol Innovation & Research Center to support our mission to become a socially and environmentally responsible company, with a vision of creating world class technology and green industries. We are committed to becoming a value adder through new initiatives, world class technology and innovation, delivering products and services that surpass the expectations of all of our stakeholders,
     and using cutting-edge knowledge to pave way for a truly sustainable business.
    """.strip(),
    file_path="question_12.txt",
)

write_file(
    question="Cane and Crop production",
    answer="""
    Our research & development efforts aim to increase the efficiency of modern cane farming management.
    Areas of research include alternative plants and biological pest control to reduce the usage of chemicals in cane farming.
    A team of experts help to distribute the knowledge to farmers.
    """.strip(),
    file_path="question_13.txt",
)

"""# Model"""

DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"

DEVICE

from transformers import AutoTokenizer, pipeline, logging
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
import argparse

model_name_or_path = "TheBloke/Nous-Hermes-13B-GPTQ"
model_basename = "model"

use_triton = False

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)

model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,
        model_basename=model_basename,
        use_safetensors=True,
        trust_remote_code=True,
        device=DEVICE,
        use_triton=use_triton,
        quantize_config=None)
generation_config = GenerationConfig.from_pretrained(model_name_or_path)

# model_name_or_path = "TheBloke/Nous-Hermes-13B-GPTQ"
# model_basename = "nous-hermes-13b-GPTQ-4bit-128g.no-act.order"

# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)
# model = AutoGPTQForCausalLM.from_quantized(
#         model_name_or_path,
#         model_basename=model_basename,
#         use_safetensors=True,
#         trust_remote_code=True,
#         device=DEVICE,
#     )
# generation_config = GenerationConfig.from_pretrained(model_name_or_path)

question = (
    "which programming language is more suitable for a beginner: Python or Javascript?"
)
prompt = f"""
###Instruction: {question}
###Response:
""".strip()

print(prompt)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(DEVICE)
# with torch.inference_mode():
#   output = model.generate(inputs = input_ids, temperature=0.7,max_new_tokens=512)

print(tokenizer.decode(output[0]))

generation_config

streamer = TextStreamer(
    tokenizer, skip_prompt=True, skip_special_tokens=True, use_multiprocessing=False
)

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_length=2048,
    temperature=0,
    top_p=0.95,
    repetition_penalty=1.15,
    generation_config=generation_config,
    streamer=streamer,
    batch_size=1,
)

llm = HuggingFacePipeline(pipeline=pipe)

response = llm(prompt)

"""# **Embed Documents**"""

embeddings = HuggingFaceEmbeddings(
    model_name="embaas/sentence-transformers-multilingual-e5-base",
    model_kwargs={"device": DEVICE},
)

loader = DirectoryLoader("./skyscanner/", glob="**/*txt")
documents = loader.load()
len(documents)

text_splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

texts[4]

db = Chroma.from_documents(texts, embeddings)

db.similarity_search("flight search")

"""# Conversational chain"""

template = """
### Instruction: You're a advisor of a company that need to give the company information to customers. Use only the chat history and the following information
{context}
to answer in a helpful manner to the question. IF you don't know the answer ~ say that you don't know.
Keep your replies short, compassionate and informative.
{chat_history}
### input: {question}
### Response:
""".strip()

prompt = PromptTemplate(
    input_variables={"context", "question", "chat_history"}, template=template
)

memory = ConversationBufferMemory(
    memory_key="chat_history",
    human_prefix="### Input",
    ai_prefix="### Response",
    output_key="answer",
    return_messages=True,
)

chain = ConversationalRetrievalChain.from_llm(
    llm,
    chain_type="stuff",
    retriever=db.as_retriever(),
    memory=memory,
    combine_docs_chain_kwargs={"prompt": prompt},
    return_source_documents=True,
    verbose=True,
)

question = "How flight search works?"
answer = chain(question)

answer.keys()

answer["source_documents"]

question = "I bought flight tickets, but I can't find any confirmation. Where is it?"
response = chain(question)

"""# QA Chain with Memory"""

memory = ConversationBufferMemory(
    memory_key="chat_history",
    human_prefix="### Input",
    ai_prefix="### Response",
    output_key="output_text",
    return_messages=False,
)
chain = load_qa_chain(
    llm, chain_type="stuff", prompt=prompt, memory=memory, verbose=True
)

"""# Support Chatbot"""

DEFAULT_TEMPLATE = """
### Instruction: You're a advisor of a company that need to give the company information to customers. Use only the chat history and the following information
{context}
to answer in a helpful manner to the question. IF you don't know the answer ~ say that you don't know.
Keep your replies short, compassionate and informative.
{chat_history}
### input: {question}
### Response:
""".strip()

class Chatbot:
  def __init__(
      self,
      text_pipeline: HuggingFacePipeline,
      embeddings: HuggingFaceEmbeddings,
      documents_dir: Path,
      prompt_template: str = DEFAULT_TEMPLATE,
      verbose: bool=False,
  ):
      prompt = PromptTemplate(
        input_variables=["context", "question", "chat_history"],
        template=prompt_template,
      )
      self.chain = self._create_chain(text_pipeline, prompt, verbose)
      self.db = self._embed_data(documents_dir, embeddings)

  def _create_chain(
      self,
      text_pipeline: HuggingFacePipeline,
      prompt: PromptTemplate,
      verbose: bool = False,
  ):
      memory = ConversationBufferMemory(
          memory_key="chat_history",
          human_prefix="### Input",
          ai_prefix="### Response",
          input_key="question",
          output_key="output_text",
          return_messages=False,
      )
      return load_qa_chain(
          text_pipeline,
          chain_type="stuff",
          prompt=prompt,
          memory=memory,
          verbose=verbose,
      )

  def _embed_data(
      self, documents_dir: Path, embeddings: HuggingFaceEmbeddings
  )-> Chroma:
      loader = DirectoryLoader(documents_dir, glob="**/*txt")
      documents = loader.load()
      text_splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=0)
      texts = text_splitter.split_documents(documents)
      return Chroma.from_documents(texts, embeddings)
  def __call__(self, user_input: str)->str:
    docs = self.db.similarity_search(user_input)
    return self.chain.run({"input_documents": docs, "question": user_input})

chatbot = Chatbot(llm, embeddings, "./skyscanner/")

# model.save_pretrained('./model')
# tokenizer.save_pretrained('./model')

# model = Automodel.from_pretrained('./model')

import warnings

warnings.filterwarnings("ignore", category=UserWarning)

while True:
  user_input = input("Type bye to exit\nYour prompt: ")
  if user_input.lower() in ["bye", "goodbye"]:
    break
  answer = chatbot(user_input)
  print()

